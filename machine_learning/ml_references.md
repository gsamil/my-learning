# References

## Various Topics

- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [Precision & Recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)
- [Activation Functions in Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
- [Understanding LSTM Networks - Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)
- [Attention? Attention! - Lil'Log Blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
- [The Best Guide to Regularization in Machine Learning](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning)

## Backpropagation

At the heart of backpropagation is an expression for the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases.

- [How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)
- [Backpropagation calculus | Chapter 4, Deep learning](https://youtu.be/tIeHLnjs5U8)
