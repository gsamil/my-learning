# My Learning Notes

I created this repo to have an organized view of them in my mind. I might not need this in future, or maybe I may need to create more, for now let's start with this.

## DevOps

- [Gitlab CI/CD](./devops/gitlab-ci-cd.md)
- [Portainer](./devops/portainer.md)
- [Docker Containers & Kubernetes Fundamentals](./devops/docker_kubernetes/README.md)
- [Graylog](./devops/graylog.md)

## BackEnd

- [Apache Kafka Tutorial](./backend/kafka.md)
- [PostgreSQL Tutorial](./backend/postgresql.md)

## Machine Learning

- [ML References](./machine_learning/references.md)
- [AWS Certified Machine Learning Specialty (Udemy Course Notes)](./machine_learning/udemy-aws-mls-c01/README.md)
- [AWS Certified Cloud Practitioner (AWS Course Notes)](./machine_learning/aws-clf-c02/README.md)

| Title | Date | Description |
| --- | --- | --- |
| [Confidence Intervals for Regression Models](./machine_learning/regression_confidence.md) | 2025-3-23 | A comparison of methods of computing confidence intervals for regression models |
| [DeepSeek](./machine_learning/deepseek.md) | 2025-3-14 | Summary documentation containing history of DeepSeek models and various URLs |
| [Deep Dive into LLMs like ChatGPT](./machine_learning/andrej_karpathy_llm_deepdive.md) | 2025-3-6 | my takeaways from the video |
| [DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters](./machine_learning/lex_friedman_deepseek.md) | 2025-3-6 | my takeaways from the video |
| [Advancing AI with Local Learning and Uncertainty Estimation](./machine_learning/test_time_adaptation.md) | 2024-12-08 | o1-preview summary of the video |
| [Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)](./machine_learning/standford_cs229_building_llms.md) | 2024-11-24 | o1-preview summary of the video |
| [CLIP - Contrastive Language-Image Pretraining](./machine_learning/clip.md) | 2024-05-05 | A brief explanation of CLIP method |
| [Approximate Nearest Neighbor Methods](./machine_learning/approximate-nearest-neighbor.md) | 2023-12-22 | This blog post explores Approximate Nearest Neighbor (ANN) methods, discussing their importance, various techniques like HNSW, LSH, ANNOY, and Spill Trees, and Python libraries for implementing these methods. |
| [Text Classification Using Class Information](./machine_learning/text-classification.md) | 2023-12-18 | How should our approach to text classfication change if our classes also have meanings. |
| [Model Deployment Strategies](./machine_learning/model-deployment-strategies.md) | 2023-12-13 | I dissect the machine learning model development lifecycle, exploring deployment strategies and techniques for effective model deployment. |
| [Alignment Problem](./machine_learning/alignment_problem.md) | 2023-08-26 | Summary of the blog [Musings on the Alignment Problem](https://aligned.substack.com/) by Jan Leike |

## Speech Processing

| Title | Date | Description |
| --- | --- | --- |
| [MFCC](./speech_processing//mfcc.md) | 2022-08-25 | Short description for MFCC |

## Statistics

| Title | Date | Description |
| --- | --- | --- |
| [Hypothesis Testing](./statistics/hypothesis-testing.md) | 2023-08-25 | Short description for hypothesis testing |
| [z-score](./statistics/z-score/z-score.md) | 2023-08-25 | Short description for z-score |

## System Design

- [Designing Machine Learning Systems](./system/chip_huyen.md)

## Coding

- [Algorithms](https://github.com/gsamil/algorithms/)
- Python
    - [Run a Function on Background](./python/run_on_background.py)
    - [Run Multiple Functions on Background](./python/run_on_background_mult.py)

## Databricks
    
- [Databricks](./databricks/readme.md)

## Paper Summaries

| Title | Author | Publish Date | Description |
| --- | --- | --- | --- |
| [LoRA: Low-Rank Adaptation of Large Language Models](./paper/lora-2021.md) | Edward Hu et. al. | 2021-10-16 | LoRA introduces a resource-efficient approach for adapting large pre-trained language models, such as GPT-3, to specific tasks without the heavy costs of traditional fine-tuning. It maintains model quality, minimizes inference latency, and facilitates quick task-switching. |
| [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](./paper/peft-2023.md) | Lingling Xu et. al. | 2023-12-19 | This paper critically reviews Parameter Efficient Fine-Tuning (PEFT) methods for large pretrained language models (PLMs), highlighting their benefits in resource-limited settings. It assesses these methods' performance, efficiency, and memory usage across tasks like natural language understanding, machine translation, and generation. |
| [PaLM 2 Technical Report](./paper/anil-2023-palm-2.md) | Rohan Anil et. al. | 2023-05-17 | We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. |
| [LIMA: Less Is More for Alignment](./paper/zhou-2023-lima.md) | Chunting Zhou et. al. | 2023-05-18 | Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages. |
| [Training Compute-Optimal Large Language Models](./paper/hoffmann-2022-training-compute-optimal-llms.md) | Jordan Hoffmann et. al. | 2022-03-29 | We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. |

