# My Learning Notes

I created this repo to have an organized view of them in my mind. I might not need this in future, or maybe I may need to create more, for now let's start with this.

## DevOps

- [Gitlab CI/CD](./devops/gitlab-ci-cd.md)
- [Portainer](./devops/portainer.md)
- [Docker Containers & Kubernetes Fundamentals](./devops/docker_kubernetes/README.md)
- [Graylog](./devops/graylog.md)

## BackEnd

- [Apache Kafka Tutorial](./backend/kafka.md)
- [PostgreSQL Tutorial](./backend/postgresql.md)

## Machine Learning

- [ML References](./machine_learning/references.md)
- [AWS Certified Machine Learning Specialty (Udemy Course Notes)](./machine_learning/udemy-aws-mls-c01/README.md)
- [AWS Certified Cloud Practitioner (AWS Course Notes)](./machine_learning/aws-clf-c02/README.md)

### Blog

- **2025-3-23 | [Confidence Intervals for Regression Models](./machine_learning/regression_confidence.md)** : A comparison of methods of computing confidence intervals for regression models
- **2025-3-14 | [DeepSeek](./machine_learning/deepseek.md)** : Summary documentation containing history of DeepSeek models and various URLs
- **2025-3-6 | [Deep Dive into LLMs like ChatGPT](./machine_learning/andrej_karpathy_llm_deepdive.md)** : my takeaways from the video
- **2025-3-6 | [DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters](./machine_learning/lex_friedman_deepseek.md)** : my takeaways from the video
- **2024-12-08 | [Advancing AI with Local Learning and Uncertainty Estimation](./machine_learning/test_time_adaptation.md)** : o1-preview summary of the video
- **2024-11-24 | [Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)](./machine_learning/standford_cs229_building_llms.md)** : o1-preview summary of the video
- **2024-05-05 | [CLIP - Contrastive Language-Image Pretraining](./machine_learning/clip.md)** : A brief explanation of CLIP method
- **2023-12-22 | [Approximate Nearest Neighbor Methods](./machine_learning/approximate-nearest-neighbor.md)** : This blog post explores Approximate Nearest Neighbor (ANN) methods, discussing their importance, various techniques like HNSW, LSH, ANNOY, and Spill Trees, and Python libraries for implementing these methods.
- **2023-12-18 | [Text Classification Using Class Information](./machine_learning/text-classification.md)** : How should our approach to text classfication change if our classes also have meanings.
- **2023-12-13 | [Model Deployment Strategies](./machine_learning/model-deployment-strategies.md)** : I dissect the machine learning model development lifecycle, exploring deployment strategies and techniques for effective model deployment.
- **2023-08-26 | [Alignment Problem](./machine_learning/alignment_problem.md)** : Summary of the blog [Musings on the Alignment Problem](https://aligned.substack.com/) by Jan Leike

## Speech Processing

- **2022-08-25 | [MFCC](./speech_processing/mfcc.md)** : Short description for MFCC

## Statistics

- **2023-08-25 | [Hypothesis Testing](./statistics/hypothesis-testing.md)** : Short description for hypothesis testing
- **2023-08-25 | [z-score](./statistics/z-score/z-score.md)** : Short description for z-score

## System Design

- [Designing Machine Learning Systems](./system/chip_huyen.md)

## Coding

- [Algorithms](https://github.com/gsamil/algorithms/)

## Paper Summaries

- **2021-10-16 | Edward Hu et. al. | [LoRA: Low-Rank Adaptation of Large Language Models](./paper/lora-2021.md)** : LoRA introduces a resource-efficient approach for adapting large pre-trained language models, such as GPT-3, to specific tasks without the heavy costs of traditional fine-tuning. It maintains model quality, minimizes inference latency, and facilitates quick task-switching.
- **2023-12-19 | Lingling Xu et. al. | [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](./paper/peft-2023.md)** : This paper critically reviews Parameter Efficient Fine-Tuning (PEFT) methods for large pretrained language models (PLMs), highlighting their benefits in resource-limited settings. It assesses these methods' performance, efficiency, and memory usage across tasks like natural language understanding, machine translation, and generation.
- **2023-05-17 | Rohan Anil et. al. | [PaLM 2 Technical Report](./paper/anil-2023-palm-2.md)** : We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM.
- **2023-05-18 | Chunting Zhou et. al. | [LIMA: Less Is More for Alignment](./paper/zhou-2023-lima.md)** : Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages.
- **2022-03-29 | Jordan Hoffmann et. al. | [Training Compute-Optimal Large Language Models](./paper/hoffmann-2022-training-compute-optimal-llms.md)** : We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.
